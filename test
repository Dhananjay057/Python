import zipfile
import os
from google.cloud import storage
from vertexai.generative_models import GenerativeModel, Part
from vertexai import init

# ---------------------------------------------
# Step 0: Init Vertex AI (Update your project ID)
# ---------------------------------------------

init(project="bhsf-ai-team-projects", location="us-east1")

# ---------------------------------------------
# Step 1: Download ZIP from GCS
# ---------------------------------------------

gcs_bucket = 'gcs-dev-use1-ai-storage-03'
gcs_zip_path = 'intake-summary/Lung.zip'
local_zip_path = '/tmp/Lung.zip'

# Initialize the Google Cloud Storage client
storage_client = storage.Client()
bucket = storage_client.bucket(gcs_bucket)
blob = bucket.blob(gcs_zip_path)
blob.download_to_filename(local_zip_path)

# ---------------------------------------------
# Step 2: Extract all PDFs and summarize each
# ---------------------------------------------

summary_outputs = []
raw_texts = []  # To store raw text extracted from PDFs
model = GenerativeModel("gemini-2.5-flash-lite")  # Specify your model

# Read the custom prompt from the prompt.txt file
with open("prompt.txt", "r") as f:
    prompt = f.read().strip()  # Strip to remove any leading/trailing whitespace

with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:
    # List all files in the ZIP and filter for PDFs
    all_files = zip_ref.namelist()
    pdf_files = [f for f in all_files if f.lower().endswith(".pdf")]
    
    total_files = len(pdf_files)
    
    for idx, pdf_file in enumerate(pdf_files, 1):
        # Log the processing status (file name and progress)
        print(f"Processing ({idx}/{total_files}): {pdf_file}")
        
        # Extract each PDF file to a temporary path
        extracted_path = f"/tmp/pdf_{idx}.pdf"
        zip_ref.extract(pdf_file, path="/tmp")
        os.rename(f"/tmp/{pdf_file}", extracted_path)

        # Read the extracted PDF file and create a Part object for Vertex AI
        with open(extracted_path, "rb") as f:
            pdf_part = Part.from_data(f.read(), mime_type="application/pdf")
        
        # Modify the prompt for the current PDF (optional customization)
        current_prompt = f"{prompt} ({os.path.basename(pdf_file)}):"

        # Generate content using Vertex AI model
        response = model.generate_content(
            [
                current_prompt,  # Custom prompt for summarization
                pdf_part,  # PDF content as Part object
            ],
            generation_config={"temperature": 0.4, "max_output_tokens": 1024},
        )
        
        # Collect the summary result
        summary_outputs.append(f"üìÑ **{pdf_file}**:\n{response.text.strip()}\n")
        
        # Extract raw text from PDF (for the raw file)
        raw_text = f"Raw text from {pdf_file}:\n\n{response.text.strip()}"  # Keep the raw text as is
        raw_texts.append(raw_text)

# ---------------------------------------------
# Step 3: Combine All Summaries and Raw Text
# ---------------------------------------------

# Combine summaries
full_summary = "\n\n".join(summary_outputs)

# Combine raw text
full_raw_text = "\n\n".join(raw_texts)

# Save the raw text to "all_pdf_texts.txt"
with open("all_pdf_texts.txt", "w") as raw_text_file:
    raw_text_file.write(full_raw_text)

# Print the combined summary and raw text
print("\nCombined Summary:\n")
print(full_summary)

print("\nCombined Raw Text has been saved to 'all_pdf_texts.txt'")


----------------------------------------------------------------------------------
scores 


import json
import re

# ---------- Utility to clean model response ----------
def clean_json_text(text):
    # Strip markdown or code block formatting like ```json ... ```
    return re.sub(r"^```(?:json)?\s*|\s*```$", "", text.strip(), flags=re.IGNORECASE)

# -------------------------------
# IMPORT LIBRARIES
# -------------------------------
from vertexai.preview.generative_models import GenerativeModel
import json

# -------------------------------
# CONFIG
# -------------------------------
MODEL_NAME = "gemini-2.5-pro"
model = GenerativeModel(MODEL_NAME)

# -------------------------------
# IMPORT SUMMARY PIPELINE
# -------------------------------
# from pdf_summary_pipeline import run_pdf_summary_pipeline

# -------------------------------
# GENERATE EVALUATION QUESTIONS
# -------------------------------
def generate_eval_questions(original_text):
    prompt = f"""
You are an expert in responsible AI and clinical summarization evaluation.

Your task is to review the following clinical document and generate two curated sets of evaluation questions:

1. **Inclusion Questions**: These assess whether a summary would include all **crucial clinical content**, such as diagnostic findings, key specialist input, test results, long-term medical context, and relevant health history. Frame questions that challenge whether the summary omits any important information or underrepresents any perspective.

2. **Alignment Questions**: These check whether a summary **accurately reflects the meaning, sequence, specificity, and facts** presented in the original document. Frame questions that assess whether the summary distorts, misrepresents, or generalizes key information.

To do this:
- First, identify the most **clinically important elements** in the document‚Äîsuch as findings, diagnoses, specialist opinions, procedures, genetic or pathology test results, and timelines.
- Then formulate **10 Inclusion Questions** and **10 Alignment Questions** based on that content. Phrase them so that a reviewer could use them to evaluate whether a summary captures the full context and preserves factual integrity.

Respond only with clean JSON in the following format (no markdown or code block wrappers):

{{
  "Inclusion Questions": ["..."],
  "Alignment Questions": ["..."]
}}

Clinical Document:
{original_text}
"""

    response = model.generate_content(prompt)
    raw_text = response.text
    cleaned_text = clean_json_text(raw_text)
    return json.loads(cleaned_text)

# -------------------------------
# SCORE THE SUMMARY
# -------------------------------
def score_summary(original_text, summary_text, questions_dict):
    prompt = f"""
You are a responsible AI evaluator. Using the original text and the summary, answer the following questions.
For each question, provide:
- A brief justification
- A score between 0 and 1

Format:
{{
  "Inclusion": [{{"question": "...", "justification": "...", "score": float}}, ...],
  "Alignment": [{{"question": "...", "justification": "...", "score": float}}, ...]
}}

Original Text:
{original_text}

Summary:
{summary_text}

Questions:
{json.dumps(questions_dict, indent=2)}
"""
    response = model.generate_content(prompt)
    raw_text = response.text
    cleaned_text = clean_json_text(raw_text)
    return json.loads(cleaned_text)

# -------------------------------
# CALCULATE FINAL SCORE
# -------------------------------
def calculate_avg(scores_list):
    return sum(item["score"] for item in scores_list) / len(scores_list)

# -------------------------------
# FULL SCORING PIPELINE
# -------------------------------
def run_scoring_pipeline(bucket_name, zip_path):
    summary_text, original_text = run_pdf_summary_pipeline(bucket_name, zip_path)

    print("\nüîç Generating Evaluation Questions...")
    questions = generate_eval_questions(original_text)

    print("\nüìä Scoring the Summary...")
    scored = score_summary(original_text, summary_text, questions)

    inclusion_score = calculate_avg(scored["Inclusion"])
    alignment_score = calculate_avg(scored["Alignment"])

    return {
        "Inclusion Questions": questions["Inclusion Questions"],
        "Alignment Questions": questions["Alignment Questions"],
        "Inclusion Score": round(inclusion_score, 2),
        "Alignment Score": round(alignment_score, 2),
        "Detailed Scores": scored
    }



# -------------------------------
# MAIN EXECUTION
# -------------------------------
if __name__ == "__main__":
    BUCKET_NAME = "gcs-dev-use1-ai-storage-03"
    ZIP_FILE_PATH = "intake-summary/Breast.zip"

    result = run_scoring_pipeline(BUCKET_NAME, ZIP_FILE_PATH)

    print("\n Inclusion Score:", result["Inclusion Score"])
    print(" Alignment Score:", result["Alignment Score"])
    print("\n Detailed Scores:")
    print(json.dumps(result["Detailed Scores"], indent=2))


_____________________________________________________________________________________

prompt- question


f"""
    You are an AI Expert tasked with reviewing an **Intake Nurse Narrative** and transforming it into a list of structured **question-answer (QA) pairs**. Additionally, you will evaluate whether the **summary** of the narrative correctly reflects its content through two distinct sets of questions: **Inclusion** and **Alignment**.

    ### üîç **Instructions:**
    
    1. **Inclusion Questions**: 
       * These questions assess whether **all critical facts** from the **Intake Nurse Narrative** are **included** in the **summary**.
       * For every important detail in the narrative (such as vital signs, medical history, symptoms, medications, allergies, etc.), create a **question** asking if that information is **present** in the summary.
       * **Generate 10 questions**. The **answer** should be whether or not the **fact is included** in the summary.
    
    2. **Alignment Questions**: 
       * These questions assess whether the **facts and meaning** of the **Intake Nurse Narrative** are **accurately represented** in the **summary**.
       * For every key detail (such as vitals, symptoms, history, medications, etc.), create a **question** asking if the **summary correctly reflects** the **detail** and its **meaning**, **sequence**, and **specificity**.
       * **Generate 10 questions**. The **answer** should be whether or not the **fact is accurately represented** in the summary.
    
    ### Example:
    
    1. **Inclusion Question**: 
       - "Does the summary include the patient's blood pressure on arrival?"
       - "Yes, the summary mentions the patient's blood pressure as 138/88 mmHg."
    
    2. **Alignment Question**: 
       - "Does the summary accurately reflect the patient's blood pressure?"
       - "Yes, the summary correctly states that the patient's blood pressure was 138/88 mmHg."
    
    ### Format:
    - **Inclusion Questions** should focus on whether key facts are present in the summary.
    - **Alignment Questions** should focus on whether the facts in the summary are accurately represented, with their correct meaning, sequence, and specificity.
    - Each question should be phrased clearly and concisely. 
    - Answers should be short, factual, and reflect the truth based strictly on the content of the narrative and summary.
    
    ### Output Format:
    Respond with a JSON array containing two sections: **Inclusion Questions** and **Alignment Questions**. Each section should contain **question-answer pairs**. Generate exactly 10 questions in each section. 
    ```json
    {{
      "Inclusion Questions": [
        {{
          "question": "What was the patient's blood pressure on arrival?",
          "answer": "138/88 mmHg"
        }},
        {{
          "question": "Is the patient allergic to any medication?",
          "answer": "No"
        }}
        // Add other inclusion questions...
      ],
      "Alignment Questions": [
        {{
          "question": "Does the summary accurately reflect the patient's blood pressure?",
          "answer": "Yes, the summary mentions the patient's blood pressure as 138/88 mmHg."
        }},
        {{
          "question": "Does the summary correctly reflect the patient's allergy status?",
          "answer": "Yes, the summary correctly states that the patient is not allergic to any medication."
        }}
        // Add other alignment questions...
      ]
    }}
    ```

    ### Clinical Narrative:
    {original_text}

    ### Summary:
    {summary_text}
    """


-------------------------------------------------------------------------------------------------------------------------

#Import Libraries
import os
import io
import zipfile
import pdfplumber
from google.cloud import storage
from vertexai.generative_models import GenerativeModel
from vertexai import init

# -----------------------------------------------------
# Step 0: Config & Init
# -----------------------------------------------------
PROJECT_ID = "bhsf-ai-team-projects"
LOCATION = "us-east1"
GCS_BUCKET = "gcs-dev-use1-ai-storage-03"
ZIP_PATH_GCS = "intake-summary/Breast.zip"
LOCAL_ZIP_PATH = "/tmp/Breast.zip"
RAW_TEXT_FILE = "all_pdf_texts.txt"
MODEL_NAME = "gemini-2.5-pro"
PROMPT_FILE = "prompt.txt"

# -----------------------------------------------------
# Init Vertex AI
# -----------------------------------------------------
def init_vertex_ai():
    init(project=PROJECT_ID, location=LOCATION)

# -----------------------------------------------------
# Download ZIP from GCS
# -----------------------------------------------------
def download_zip_from_gcs(bucket_name, blob_path, destination_path):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    blob.download_to_filename(destination_path)
    print(f"‚úÖ Downloaded ZIP to: {destination_path}")

# -----------------------------------------------------
# Extract raw text from PDFs using pdfplumber
# -----------------------------------------------------
def extract_raw_text_from_zip(zip_path):
    raw_texts = []

    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        pdf_files = [f for f in zip_ref.namelist() if f.lower().endswith(".pdf")]
        
        for idx, pdf_file in enumerate(pdf_files, 1):
            print(f"üìÑ Extracting text ({idx}/{len(pdf_files)}): {pdf_file}")
            
            with zip_ref.open(pdf_file) as f:
                pdf_bytes = f.read()

            text = ""
            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"

            raw_entry = f"==== {pdf_file} ====\n{text.strip()}"
            raw_texts.append(raw_entry)

    return raw_texts

# -----------------------------------------------------
# Save raw text to file
# -----------------------------------------------------
def save_text_to_file(text, filename):
    with open(filename, "w") as f:
        f.write(text)
    print(f"üíæ Saved raw text to: {filename}")

# -----------------------------------------------------
# Generate summary using Vertex AI
# -----------------------------------------------------
def generate_summary(prompt_text, full_raw_text):
    model = GenerativeModel(MODEL_NAME)
    combined_prompt = f"{prompt_text}\n\n{full_raw_text}"

    response = model.generate_content(
        combined_prompt,
        generation_config={"temperature": 0.4},
    )

    return response.text.strip()

# -----------------------------------------------------
# Load prompt from file
# -----------------------------------------------------
def load_prompt(prompt_path):
    with open(prompt_path, "r") as f:
        return f.read().strip()

# -----------------------------------------------------
# Main pipeline runner
# -----------------------------------------------------
def run_pipeline():
    init_vertex_ai()
    download_zip_from_gcs(GCS_BUCKET, ZIP_PATH_GCS, LOCAL_ZIP_PATH)
    
    raw_texts = extract_raw_text_from_zip(LOCAL_ZIP_PATH)
    full_raw_text = "\n\n".join(raw_texts)
    
    save_text_to_file(full_raw_text, RAW_TEXT_FILE)

    prompt = load_prompt(PROMPT_FILE)
    print("‚ú® Generating combined summary...\n")
    summary = generate_summary(prompt, full_raw_text)
    
    print("üìù Combined Summary:\n")
    print(summary)

# -----------------------------------------------------
# Entry point
# -----------------------------------------------------
if __name__ == "__main__":
    run_pipeline()


This is summary code  but can you wite a code for llm as a judge for the summary using gemini-2.5-pro
for thr discrepencies invoved in summary from the original text if any ? Evaluate for factual accuracy, completeness,hallucination, clarity 
Score it from 1-10 and explain why 
