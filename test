import zipfile
import os
from google.cloud import storage
from vertexai.generative_models import GenerativeModel, Part
from vertexai import init

# ---------------------------------------------
# Step 0: Init Vertex AI (Update your project ID)
# ---------------------------------------------

init(project="bhsf-ai-team-projects", location="us-east1")

# ---------------------------------------------
# Step 1: Download ZIP from GCS
# ---------------------------------------------

gcs_bucket = 'gcs-dev-use1-ai-storage-03'
gcs_zip_path = 'intake-summary/Lung.zip'
local_zip_path = '/tmp/Lung.zip'

# Initialize the Google Cloud Storage client
storage_client = storage.Client()
bucket = storage_client.bucket(gcs_bucket)
blob = bucket.blob(gcs_zip_path)
blob.download_to_filename(local_zip_path)

# ---------------------------------------------
# Step 2: Extract all PDFs and summarize each
# ---------------------------------------------

summary_outputs = []
raw_texts = []  # To store raw text extracted from PDFs
model = GenerativeModel("gemini-2.5-flash-lite")  # Specify your model

# Read the custom prompt from the prompt.txt file
with open("prompt.txt", "r") as f:
    prompt = f.read().strip()  # Strip to remove any leading/trailing whitespace

with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:
    # List all files in the ZIP and filter for PDFs
    all_files = zip_ref.namelist()
    pdf_files = [f for f in all_files if f.lower().endswith(".pdf")]
    
    total_files = len(pdf_files)
    
    for idx, pdf_file in enumerate(pdf_files, 1):
        # Log the processing status (file name and progress)
        print(f"Processing ({idx}/{total_files}): {pdf_file}")
        
        # Extract each PDF file to a temporary path
        extracted_path = f"/tmp/pdf_{idx}.pdf"
        zip_ref.extract(pdf_file, path="/tmp")
        os.rename(f"/tmp/{pdf_file}", extracted_path)

        # Read the extracted PDF file and create a Part object for Vertex AI
        with open(extracted_path, "rb") as f:
            pdf_part = Part.from_data(f.read(), mime_type="application/pdf")
        
        # Modify the prompt for the current PDF (optional customization)
        current_prompt = f"{prompt} ({os.path.basename(pdf_file)}):"

        # Generate content using Vertex AI model
        response = model.generate_content(
            [
                current_prompt,  # Custom prompt for summarization
                pdf_part,  # PDF content as Part object
            ],
            generation_config={"temperature": 0.4, "max_output_tokens": 1024},
        )
        
        # Collect the summary result
        summary_outputs.append(f"üìÑ **{pdf_file}**:\n{response.text.strip()}\n")
        
        # Extract raw text from PDF (for the raw file)
        raw_text = f"Raw text from {pdf_file}:\n\n{response.text.strip()}"  # Keep the raw text as is
        raw_texts.append(raw_text)

# ---------------------------------------------
# Step 3: Combine All Summaries and Raw Text
# ---------------------------------------------

# Combine summaries
full_summary = "\n\n".join(summary_outputs)

# Combine raw text
full_raw_text = "\n\n".join(raw_texts)

# Save the raw text to "all_pdf_texts.txt"
with open("all_pdf_texts.txt", "w") as raw_text_file:
    raw_text_file.write(full_raw_text)

# Print the combined summary and raw text
print("\nCombined Summary:\n")
print(full_summary)

print("\nCombined Raw Text has been saved to 'all_pdf_texts.txt'")


----------------------------------------------------------------------------------
scores 


import json
import re

# ---------- Utility to clean model response ----------
def clean_json_text(text):
    # Strip markdown or code block formatting like ```json ... ```
    return re.sub(r"^```(?:json)?\s*|\s*```$", "", text.strip(), flags=re.IGNORECASE)

# -------------------------------
# IMPORT LIBRARIES
# -------------------------------
from vertexai.preview.generative_models import GenerativeModel
import json

# -------------------------------
# CONFIG
# -------------------------------
MODEL_NAME = "gemini-2.5-pro"
model = GenerativeModel(MODEL_NAME)

# -------------------------------
# IMPORT SUMMARY PIPELINE
# -------------------------------
# from pdf_summary_pipeline import run_pdf_summary_pipeline

# -------------------------------
# GENERATE EVALUATION QUESTIONS
# -------------------------------
def generate_eval_questions(original_text):
    prompt = f"""
You are an expert in responsible AI and clinical summarization evaluation.

Your task is to review the following clinical document and generate two curated sets of evaluation questions:

1. **Inclusion Questions**: These assess whether a summary would include all **crucial clinical content**, such as diagnostic findings, key specialist input, test results, long-term medical context, and relevant health history. Frame questions that challenge whether the summary omits any important information or underrepresents any perspective.

2. **Alignment Questions**: These check whether a summary **accurately reflects the meaning, sequence, specificity, and facts** presented in the original document. Frame questions that assess whether the summary distorts, misrepresents, or generalizes key information.

To do this:
- First, identify the most **clinically important elements** in the document‚Äîsuch as findings, diagnoses, specialist opinions, procedures, genetic or pathology test results, and timelines.
- Then formulate **10 Inclusion Questions** and **10 Alignment Questions** based on that content. Phrase them so that a reviewer could use them to evaluate whether a summary captures the full context and preserves factual integrity.

Respond only with clean JSON in the following format (no markdown or code block wrappers):

{{
  "Inclusion Questions": ["..."],
  "Alignment Questions": ["..."]
}}

Clinical Document:
{original_text}
"""

    response = model.generate_content(prompt)
    raw_text = response.text
    cleaned_text = clean_json_text(raw_text)
    return json.loads(cleaned_text)

# -------------------------------
# SCORE THE SUMMARY
# -------------------------------
def score_summary(original_text, summary_text, questions_dict):
    prompt = f"""
You are a responsible AI evaluator. Using the original text and the summary, answer the following questions.
For each question, provide:
- A brief justification
- A score between 0 and 1

Format:
{{
  "Inclusion": [{{"question": "...", "justification": "...", "score": float}}, ...],
  "Alignment": [{{"question": "...", "justification": "...", "score": float}}, ...]
}}

Original Text:
{original_text}

Summary:
{summary_text}

Questions:
{json.dumps(questions_dict, indent=2)}
"""
    response = model.generate_content(prompt)
    raw_text = response.text
    cleaned_text = clean_json_text(raw_text)
    return json.loads(cleaned_text)

# -------------------------------
# CALCULATE FINAL SCORE
# -------------------------------
def calculate_avg(scores_list):
    return sum(item["score"] for item in scores_list) / len(scores_list)

# -------------------------------
# FULL SCORING PIPELINE
# -------------------------------
def run_scoring_pipeline(bucket_name, zip_path):
    summary_text, original_text = run_pdf_summary_pipeline(bucket_name, zip_path)

    print("\nüîç Generating Evaluation Questions...")
    questions = generate_eval_questions(original_text)

    print("\nüìä Scoring the Summary...")
    scored = score_summary(original_text, summary_text, questions)

    inclusion_score = calculate_avg(scored["Inclusion"])
    alignment_score = calculate_avg(scored["Alignment"])

    return {
        "Inclusion Questions": questions["Inclusion Questions"],
        "Alignment Questions": questions["Alignment Questions"],
        "Inclusion Score": round(inclusion_score, 2),
        "Alignment Score": round(alignment_score, 2),
        "Detailed Scores": scored
    }



# -------------------------------
# MAIN EXECUTION
# -------------------------------
if __name__ == "__main__":
    BUCKET_NAME = "gcs-dev-use1-ai-storage-03"
    ZIP_FILE_PATH = "intake-summary/Breast.zip"

    result = run_scoring_pipeline(BUCKET_NAME, ZIP_FILE_PATH)

    print("\n Inclusion Score:", result["Inclusion Score"])
    print(" Alignment Score:", result["Alignment Score"])
    print("\n Detailed Scores:")
    print(json.dumps(result["Detailed Scores"], indent=2))
